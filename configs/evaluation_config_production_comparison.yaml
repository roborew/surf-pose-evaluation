# Production Model Comparison Configuration
# Note: Output paths (mlflow, predictions, visualizations, best_params) are automatically
# managed by RunManager when using run_production_evaluation.py. They are organized in
# timestamped folders under results/runs/{timestamp}_{name}/.

# Data Source Configuration - Production paths with FFV1 format
data_source:
  base_data_path: "./data/SD_02_SURF_FOOTAGE_PREPT"
  video_clips:
    h264_path: "03_CLIPPED/h264"
    ffv1_path: "03_CLIPPED/ffv1"
    input_format: "ffv1" # Production uses ffv1 format
  annotations:
    labels_path: "04_ANNOTATED/surf-manoeuvre-labels"
    maneuver_annotations_file: "maneuver_annotations.json"
    sony_300_labels: "sony_300"
    sony_70_labels: "sony_70"
  splits:
    train_file: "05_ANALYSED_DATA/POSE/splits/train_split.json"
    val_file: "05_ANALYSED_DATA/POSE/splits/val_split.json"
    test_file: "05_ANALYSED_DATA/POSE/splits/test_split.json"
    train_ratio: 0.70
    val_ratio: 0.15
    test_ratio: 0.15
    random_seed: 42
    zoom_handling:
      enabled: true
      balanced_distribution: true
      force_no_zoom_in_val: true
  camera_selection:
    enabled_cameras: ["SONY_300", "SONY_70"] # Production uses both cameras
    max_clips_per_session: 200 # Full dataset for comparison

# Model Configuration
models:
  mediapipe:
    config_path: "configs/model_configs/mediapipe.yaml"
  blazepose:
    config_path: "configs/model_configs/blazepose.yaml"
  yolov8_pose:
    config_path: "configs/model_configs/yolov8_pose.yaml"
  pytorch_pose:
    config_path: "configs/model_configs/pytorch_pose.yaml"
  mmpose:
    config_path: "configs/model_configs/mmpose.yaml"

# Evaluation Configuration
evaluation:
  quick_test:
    enabled: true
    num_clips: 50
    models: ["yolov8_pose", "pytorch_pose", "mmpose"] # Quick validation with reliable models
  comprehensive_test:
    enabled: true
    num_clips: 200 # Full dataset for comparison
    models: ["mediapipe", "blazepose", "yolov8_pose", "pytorch_pose", "mmpose"]
    use_best_params: true # Use parameters from Optuna phase

# Optuna Configuration
optuna:
  enabled: false # Disabled for comparison phase

# Predictions Configuration - ENABLED for final model comparison
predictions:
  enabled: true
  save_detailed_predictions: true
  include_confidence_scores: true
  include_bounding_boxes: true

# Visualizations Configuration - ENABLED for final model comparison
visualizations:
  enabled: true
  save_overlay_videos: true
  save_keypoint_plots: true
  save_comparison_plots: true
  max_examples_per_model: 10 # Generate comprehensive visualizations

# Performance Configuration (Production optimized)
performance:
  max_workers: 8 # Full utilization for production
  batch_size: 1
  device: "auto" # Auto-detect: CUDA first, then MPS/CPU
  memory_limit_gb: 32
  enable_gpu_memory_growth: true
  half_precision: true # Enable FP16 for RTX 4090

# Minimal configuration for direct usage with evaluate_pose_models.py
# (RunManager will override these with proper paths when using run_production_evaluation.py)
mlflow:
  enabled: true
  experiment_name: "surf_pose_production_comparison"

output:
  predictions:
    enabled: true
  visualization:
    enabled: true
# Reference configurations (commented out - handled by RunManager or rarely changed)
#
# experiment:
#   name: "surf_pose_production_comparison"
#   description: "Final model comparison using optimal hyperparameters on full dataset"
#   version: "1.0"
#   author: "Production Team"
#
# dataset:
#   base_data_path: "./data/SD_02_SURF_FOOTAGE_PREPT"
#   video_clips:
#     input_format: "ffv1"  # Production uses ffv1 format
#   annotations:
#     labels_path: "04_ANNOTATED/surf-manoeuvre-labels"
#   splits:
#     train_ratio: 0.70
#     val_ratio: 0.15
#     test_ratio: 0.15
#     random_seed: 42
#
# models:
#   load_best_params:
#     enabled: true
#     source_path: "./results/best_params"
#     fallback_to_defaults: true
#
# evaluation:
#   metrics:
#     pose_accuracy:
#       - "pck_0.1"
#       - "pck_0.2"
#       - "pck_0.3"
#       - "mpjpe"
#     performance:
#       - "inference_latency_ms"
#       - "memory_usage_mb"
#       - "throughput_fps"
#       - "model_size_mb"
#
# hardware:
#   gpu:
#     device_id: 0
#     memory_fraction: 0.9
#     allow_growth: true
#   cpu:
#     num_workers: 8
#     num_threads: 16
#   memory:
#     max_memory_gb: 32
#     cache_enabled: true
#     clear_cache_frequency: 50
#
# output:
#   # High-quality video encoding for final results
#   visualization:
#     encoding:
#       format: "h264"
#       quality:
#         crf: 23
#         preset: "fast"
#       pixel_format: "yuv420p"
#       audio:
#         enabled: true
#         codec: "copy"
#       container: "mp4"
#
#   reports:
#     generate_html_report: true
#     generate_pdf_summary: true
#     include_failure_analysis: true
#
# logging:
#   level: "INFO"
#   format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
#   save_to_file: true
#
# debug:
#   enabled: false
#   save_intermediate_results: false
#   profile_memory: true
#   profile_compute: true
#   verbose_errors: true
