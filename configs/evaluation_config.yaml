# Main Evaluation Configuration for Surfing Pose Estimation Comparison
# This file controls all aspects of the pose model evaluation pipeline

# Experiment metadata
experiment:
  name: "surf_pose_backbone_comparison"
  description: "Systematic evaluation of pose estimation libraries for surfing action recognition"
  version: "1.0"
  author: "Your Name"

# Dataset configuration
dataset:
  # Base paths (relative to project root)
  base_data_path: "./data/SD_02_SURF_FOOTAGE_PREPT"

  # Video sources
  video_clips:
    h264_path: "03_CLIPPED/h264"
    ffv1_path: "03_CLIPPED/ffv1" # Available on training machines

  # Annotation sources
  annotations:
    labels_path: "04_ANNOTATED/surf-manoeuvre-labels"
    sony_300_labels: "sony_300"
    sony_70_labels: "sony_70"

  # Data splits for evaluation
  splits:
    train_ratio: 0.70
    val_ratio: 0.15
    test_ratio: 0.15
    random_seed: 42

    # Zoom variation handling to prevent data leakage
    zoom_handling:
      enabled: true
      balanced_distribution: true
      target_distribution:
        default: 0.33 # ~33% of clips use default zoom (C0019_clip_1.mp4)
        wide: 0.33 # ~33% of clips use wide zoom (C0019_clip_1_wide.mp4)
        full: 0.34 # ~34% of clips use full zoom (C0019_clip_1_full.mp4)
      randomized_selection: true # Randomize zoom selection within balanced constraints

  # Video processing settings
  video:
    target_fps: 25
    max_resolution: [1920, 1080]
    min_resolution: [640, 480]
    supported_formats: [".mp4", ".avi", ".mov", ".mkv"]

# Model configuration
models:
  # Models to evaluate
  enabled_models:
    - "mediapipe"
    - "blazepose"
    - "yolov8_pose"
    # Note: Enable these after proper installation:
    # - "mmpose"    # Requires: pip install mmengine mmcv mmdet mmpose
    # - "hrnet"     # Requires: MMPose + additional HRNet setup
    # - "openpose"  # Enable if OpenPose is properly installed

  # Model-specific settings (detailed configs in model_configs/)
  default_settings:
    confidence_threshold: 0.3
    batch_size: 1 # Most pose models work best with batch_size=1
    device: "cuda" # "cuda" or "cpu"
    half_precision: false # Enable for memory optimization

  # Performance optimization
  optimization:
    use_tensorrt: false # Enable on compatible systems
    use_onnx: false # ONNX optimization
    compile_models: false # PyTorch 2.0 compile

# Evaluation configuration
evaluation:
  # Test subsets for different evaluation phases
  quick_test:
    num_clips: 50
    max_duration_seconds: 10
    cameras: ["SONY_300", "SONY_70"]

  comprehensive_test:
    num_clips: 300
    max_duration_seconds: 30
    cameras: ["SONY_300", "SONY_70"]
    sessions: ["SESSION_060325", "SESSION_070325", "SESSION_080325"]

  # Metrics to compute
  metrics:
    pose_accuracy:
      - "pck_0.1" # PCK at 10% threshold
      - "pck_0.2" # PCK at 20% threshold (primary metric)
      - "pck_0.3" # PCK at 30% threshold
      - "mpjpe" # Mean Per Joint Position Error (3D models)

    performance:
      - "inference_latency_ms"
      - "memory_usage_gb"
      - "throughput_fps"
      - "model_size_mb"

    temporal:
      - "temporal_consistency"
      - "pose_smoothness"
      - "missing_detections_rate"

  # Ground truth sources for comparison
  ground_truth:
    use_manual_annotations: true
    use_cross_validation: true
    minimum_keypoint_visibility: 0.5

# MLflow configuration
mlflow:
  enabled: true
  experiment_name: "surf_pose_comparison"
  tracking_uri: "./results/mlruns"

  # What to log
  logging:
    parameters: true
    metrics: true
    artifacts: true
    models: false # Disable model logging initially to save space

    # Artifact logging
    log_predictions: true
    log_visualizations: true
    max_videos_per_run: 5

  # Tags for organization
  default_tags:
    project: "surf_pose_evaluation"
    stage: "research"
    dataset: "widemouth_march_2025"

# Optuna hyperparameter optimization
optuna:
  enabled: true # Enable for hyperparameter optimization
  study_name: "surf_pose_optimization_production"
  direction: "maximize" # Maximize PCK@0.2
  n_trials: 50 # Balanced for production (more thorough than macOS but not excessive)
  timeout_minutes: 180 # 3 hours max (reasonable for production)

  # Optimization target
  objective:
    primary_metric: "pck_0.2"
    secondary_metrics: ["inference_latency_ms", "memory_usage_gb"]
    weights: [0.7, 0.2, 0.1] # Primary, speed, memory

  # Pruning for early stopping
  pruning:
    enabled: true
    patience: 3 # Efficient early stopping (improved from 10)
    min_trials: 5 # Quick start for pruning (improved from 20)

# Hardware configuration
hardware:
  # GPU settings
  gpu:
    device_id: 0
    memory_fraction: 0.9 # Use 90% of available GPU memory
    allow_growth: true

  # CPU settings
  cpu:
    num_workers: 4
    num_threads: 8

  # Memory management
  memory:
    max_memory_gb: 16
    cache_enabled: true
    clear_cache_frequency: 50 # Clear every N clips

# Visualization and output
output:
  # Results directory
  results_dir: "./results"

  # Visualization settings
  visualization:
    enabled: true
    save_overlay_videos: true
    save_keypoint_plots: true
    save_comparison_plots: true
    max_examples_per_model: 10

  # Report generation
  reports:
    generate_html_report: true
    generate_pdf_summary: false
    include_failure_analysis: true

# Logging configuration
logging:
  level: "INFO" # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  save_to_file: true
  log_file: "./results/evaluation.log"

# Development and debugging
debug:
  enabled: false
  save_intermediate_results: false
  profile_memory: false
  profile_compute: false
  verbose_errors: true
